{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e346c73-7a50-42e6-bb75-01260f387a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SpeechRecognition\n",
      "  Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SpeechRecognition) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SpeechRecognition) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n",
      "Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl (32.8 MB)\n",
      "   ---------------------------------------- 0.0/32.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/32.8 MB 13.4 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 3.9/32.8 MB 10.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 7.1/32.8 MB 12.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 10.0/32.8 MB 12.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 13.1/32.8 MB 13.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 16.3/32.8 MB 13.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 19.4/32.8 MB 13.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 22.5/32.8 MB 13.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 25.2/32.8 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 28.0/32.8 MB 13.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 31.2/32.8 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 14.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 14.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 14.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 14.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 32.8/32.8 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.11.0 pydub-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\nurja\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4cfb55a-39ab-45a2-86cc-e3435e4a7f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found: C:\\Users\\nurja\\research\\Recording.m4a\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "audio_path = r\"C:\\Users\\nurja\\research\\Recording.m4a\"  # Replace with your actual file path\n",
    "\n",
    "if os.path.exists(audio_path):\n",
    "    print(\"File found:\", audio_path)\n",
    "else:\n",
    "    print(\"File not found at:\", audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c499e850-8c68-41fa-a682-7a0f41bd41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "\n",
    "# Replace this path with the actual path to ffmpeg.exe\n",
    "AudioSegment.converter = \"C:\\\\ffmpeg-7.1-full_build\\\\bin\\\\ffmpeg.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1672cee-e66f-4b83-8dcd-64fb3d607bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def convert_audio_to_wav(audio_file):\n",
    "    try:\n",
    "        print(f\"Converting file: {audio_file}\")\n",
    "        audio = AudioSegment.from_file(audio_file, format=\"m4a\")\n",
    "        wav_file = audio_file.replace(\".m4a\", \".wav\")\n",
    "        audio.export(wav_file, format=\"wav\")\n",
    "        print(f\"Converted successfully to: {wav_file}\")\n",
    "        return wav_file\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found during conversion.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during conversion: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c346290-5e12-409e-9cc6-a29310e1ef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting file: C:\\Users\\nurja\\research\\Recording.m4a\n",
      "File not found during conversion.\n"
     ]
    }
   ],
   "source": [
    "wav_file_path = convert_audio_to_wav(audio_path)\n",
    "if wav_file_path:\n",
    "    transcription = convert_audio_to_text(wav_file_path)\n",
    "    print(\"Transcription:\\n\", transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f06f6958-5a43-4724-a804-c047cb3cc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio...\n",
      "Transcribing audio...\n",
      "Transcription:\n",
      " Error with the speech recognition service: recognition request failed: Bad Request\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    try:\n",
    "        with sr.AudioFile(audio_file) as source:\n",
    "            print(\"Processing audio...\")\n",
    "            audio_data = recognizer.record(source)\n",
    "        print(\"Transcribing audio...\")\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(\"Transcription complete.\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Unable to understand the audio.\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Error with the speech recognition service: {e}\"\n",
    "\n",
    "# Path to the converted .wav file\n",
    "wav_file_path = r\"C:\\Users\\nurja\\research\\Recording.wav\"\n",
    "\n",
    "# Transcribe the .wav file\n",
    "transcription = transcribe_audio(wav_file_path)\n",
    "print(\"Transcription:\\n\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9777f9a-c61c-4a58-93aa-d7a25df160e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.11.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SpeechRecognition) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SpeechRecognition) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\nurja\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade SpeechRecognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8ecf15c-e9d2-4040-9951-6bce6cec4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio file: C:\\Users\\nurja\\research\\Recording_fixed.wav\n",
      "Processing audio...\n",
      "Audio processed, starting transcription...\n",
      "Error with the speech recognition service: recognition request failed: Bad Request\n",
      "Transcription:\n",
      " Error with the speech recognition service: recognition request failed: Bad Request\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    try:\n",
    "        print(f\"Loading audio file: {audio_file}\")\n",
    "        with sr.AudioFile(audio_file) as source:\n",
    "            print(\"Processing audio...\")\n",
    "            audio_data = recognizer.record(source)\n",
    "        print(\"Audio processed, starting transcription...\")\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(\"Transcription complete.\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio.\")\n",
    "        return \"Unable to understand the audio.\"\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Error with the speech recognition service: {e}\")\n",
    "        return f\"Error with the speech recognition service: {e}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return f\"Unexpected error: {e}\"\n",
    "\n",
    "# Path to the converted .wav file\n",
    "wav_file_path = r\"C:\\Users\\nurja\\research\\Recording_fixed.wav\"\n",
    "\n",
    "# Transcribe the audio\n",
    "transcription = transcribe_audio(wav_file_path)\n",
    "print(\"Transcription:\\n\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fad9b169-cc44-466a-b56e-ed528e85bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.gitNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\nurja\\AppData\\Local\\Temp\\pip-req-build-xwvxuv8p'\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\nurja\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\nurja\\appdata\\local\\temp\\pip-req-build-xwvxuv8p\n",
      "  Resolved https://github.com/openai/whisper.git to commit 173ff7dd1d9fb1c4fddea0d41d704cfefeb8908c\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numba (from openai-whisper==20240930)\n",
      "  Downloading numba-0.60.0-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai-whisper==20240930) (1.26.4)\n",
      "Collecting torch (from openai-whisper==20240930)\n",
      "  Downloading torch-2.5.1-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai-whisper==20240930) (4.66.5)\n",
      "Collecting more-itertools (from openai-whisper==20240930)\n",
      "  Downloading more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting tiktoken (from openai-whisper==20240930)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->openai-whisper==20240930)\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->openai-whisper==20240930)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
      "Collecting filelock (from torch->openai-whisper==20240930)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
      "Collecting networkx (from torch->openai-whisper==20240930)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
      "Collecting fsspec (from torch->openai-whisper==20240930)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch->openai-whisper==20240930)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->openai-whisper==20240930)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->openai-whisper==20240930) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nurja\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch->openai-whisper==20240930) (2.1.5)\n",
      "Downloading more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Downloading numba-0.60.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.6/2.7 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 8.2 MB/s eta 0:00:00\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-win_amd64.whl (884 kB)\n",
      "   ---------------------------------------- 0.0/884.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 884.2/884.2 kB 5.7 MB/s eta 0:00:00\n",
      "Downloading torch-2.5.1-cp310-cp310-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.6/203.1 MB 13.8 MB/s eta 0:00:15\n",
      "    --------------------------------------- 5.0/203.1 MB 11.6 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 7.6/203.1 MB 12.7 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 10.2/203.1 MB 12.5 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 12.6/203.1 MB 12.3 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 14.7/203.1 MB 12.1 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 17.8/203.1 MB 12.4 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 20.2/203.1 MB 12.3 MB/s eta 0:00:15\n",
      "   ---- ----------------------------------- 22.8/203.1 MB 12.3 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 25.7/203.1 MB 12.5 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 27.8/203.1 MB 12.3 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 29.9/203.1 MB 12.2 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 32.5/203.1 MB 12.2 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 34.3/203.1 MB 12.3 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 37.7/203.1 MB 12.2 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 40.6/203.1 MB 12.5 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 43.3/203.1 MB 12.5 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 46.4/203.1 MB 12.6 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 49.3/203.1 MB 12.7 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 51.9/203.1 MB 12.7 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 54.3/203.1 MB 12.7 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 56.6/203.1 MB 12.6 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 59.8/203.1 MB 12.7 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 62.7/203.1 MB 12.7 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 65.5/203.1 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 67.9/203.1 MB 12.7 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 71.0/203.1 MB 12.8 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 73.4/203.1 MB 12.8 MB/s eta 0:00:11\n",
      "   -------------- ------------------------- 74.7/203.1 MB 12.5 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 77.3/203.1 MB 12.6 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 79.7/203.1 MB 12.5 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 82.1/203.1 MB 12.5 MB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 85.2/203.1 MB 12.6 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 87.3/203.1 MB 12.6 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 87.8/203.1 MB 12.3 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 93.8/203.1 MB 12.7 MB/s eta 0:00:09\n",
      "   ------------------- -------------------- 97.0/203.1 MB 12.7 MB/s eta 0:00:09\n",
      "   ------------------- -------------------- 99.9/203.1 MB 12.8 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 102.8/203.1 MB 12.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 105.9/203.1 MB 12.9 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 106.7/203.1 MB 12.7 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 107.2/203.1 MB 12.4 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 107.7/203.1 MB 12.2 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 108.3/203.1 MB 11.9 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 116.1/203.1 MB 12.5 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 119.0/203.1 MB 12.5 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 121.1/203.1 MB 12.6 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 125.0/203.1 MB 12.6 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 127.9/203.1 MB 12.6 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 130.5/203.1 MB 12.7 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 133.2/203.1 MB 12.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 135.8/203.1 MB 12.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 138.7/203.1 MB 12.7 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 139.7/203.1 MB 12.6 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 143.4/203.1 MB 12.6 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 146.0/203.1 MB 12.6 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 149.2/203.1 MB 12.6 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 151.8/203.1 MB 12.6 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 154.7/203.1 MB 12.7 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 156.8/203.1 MB 12.6 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 157.3/203.1 MB 12.5 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 157.8/203.1 MB 12.3 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 158.3/203.1 MB 12.1 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 158.9/203.1 MB 12.0 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 159.4/203.1 MB 11.8 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 159.6/203.1 MB 11.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 166.7/203.1 MB 12.0 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 168.8/203.1 MB 12.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 171.7/203.1 MB 12.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 174.1/203.1 MB 12.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 176.9/203.1 MB 12.0 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 179.8/203.1 MB 12.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 181.9/203.1 MB 12.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 185.3/203.1 MB 12.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 188.5/203.1 MB 12.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 191.1/203.1 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 193.5/203.1 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 196.3/203.1 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.0/203.1 MB 12.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  201.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 203.1/203.1 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 14.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.2/6.2 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.2 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.43.0-cp310-cp310-win_amd64.whl (28.1 MB)\n",
      "   ---------------------------------------- 0.0/28.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.9/28.1 MB 13.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 5.2/28.1 MB 12.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 8.1/28.1 MB 13.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 11.0/28.1 MB 13.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 13.4/28.1 MB 12.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 16.3/28.1 MB 13.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 19.1/28.1 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 21.5/28.1 MB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 24.1/28.1 MB 12.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.0/28.1 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.0/28.1 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.0/28.1 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.0/28.1 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.1/28.1 MB 10.3 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.6/1.7 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.8 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=812903 sha256=a91c0cb659dd99c3dd01542290664780166bf093b16d088cd350be03a14423a3\n",
      "  Stored in directory: C:\\Users\\nurja\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-hlvxnny3\\wheels\\8b\\6c\\d0\\622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: mpmath, sympy, regex, networkx, more-itertools, llvmlite, fsspec, filelock, torch, tiktoken, numba, openai-whisper\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 llvmlite-0.43.0 more-itertools-10.5.0 mpmath-1.3.0 networkx-3.4.2 numba-0.60.0 openai-whisper-20240930 regex-2024.11.6 sympy-1.13.1 tiktoken-0.8.0 torch-2.5.1\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "789d9ac2-0926-4d5c-9b03-7b8775abeaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper installed successfully!\n",
      "FFmpeg not found. Make sure it is installed and added to PATH.\n",
      "Audio file found: C:\\Users\\nurja\\research\\Recording_fixed.wav\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Test Whisper installation\n",
    "try:\n",
    "    model = whisper.load_model(\"base\")\n",
    "    print(\"Whisper installed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Whisper model: {e}\")\n",
    "\n",
    "# Test FFmpeg installation\n",
    "try:\n",
    "    result = subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"FFmpeg installed successfully!\")\n",
    "    else:\n",
    "        print(\"FFmpeg is not properly installed or not in PATH.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FFmpeg not found. Make sure it is installed and added to PATH.\")\n",
    "\n",
    "# Check if audio file exists\n",
    "audio_path = r\"C:\\Users\\nurja\\research\\Recording_fixed.wav\"\n",
    "if os.path.exists(audio_path):\n",
    "    print(\"Audio file found:\", audio_path)\n",
    "else:\n",
    "    print(\"Audio file not found:\", audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b887a52-79c3-4424-a4c3-7daec0ee79a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg is accessible in Python!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check FFmpeg availability in Python\n",
    "try:\n",
    "    result = subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"FFmpeg is accessible in Python!\")\n",
    "    else:\n",
    "        print(\"FFmpeg is not accessible in Python.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FFmpeg not found in Python environment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26f9ab7c-f41e-4510-8521-40589d999690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFmpeg is now accessible in Python!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Add FFmpeg path to the environment\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\ffmpeg-7.1-full_build\\bin\"  # Replace with the actual path to FFmpeg bin\n",
    "\n",
    "# Verify FFmpeg in Python\n",
    "import subprocess\n",
    "result = subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"FFmpeg is now accessible in Python!\")\n",
    "else:\n",
    "    print(\"FFmpeg is still not accessible in Python.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78c20d9b-a3a6-4101-bf72-5c66b921434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription:\n",
      "  by nine human. So I have a, okay. Time is a post doctoral researcher in the Department of Computer Science, Johns Hopkins University. He's very works with Ramon Aurora. He is broadly interested in the theoretical and algorithmic conditions of machine learning for modern partnerships and intelligence. We turn at research spanning reinforcement learning, transfer learning, multi-agent learning, trustworthy AI, and large language models. He's in Harriet chair for ASTAPS 2025 and I've seen your program committee mentor, Portraitable AI 25, and five, four, three completed speed and the wider digital tellers can you do the university in Australia, where you would order the output of the mental, for the most outstanding thesis. He then figured out there's a lot of spiders on Australia and he moved. That was not part of the time. That's it. So that's that's only. As responsibilities foracho and development as well as we're interdisciplinary students. Thank you very much. To the top presenterumbles and companies that you leave the highest of per dasame pens, the computer study and given day by day by day. Thank you so much. Ladies and gentlemen. the data drive and decision-making in Kotlin and data driven, this will make us not moving. If you try to click on the data, I don't think I can get this. But I have a few other smoothies. I think you have to look at this game here. You you basically And the embossment have actually been impressive. In clinical success, I will vote with a wide range of applications from a plate of e-mgans at the same level as humans since the passing in the name of goal or defeating top-ranked person or player in stuff. So I believe the lesson we can learn from the past because it's that we can do a embossment pretty well in the last-camping problems. It will use risk function approximation such as ill-equor and we can throw in a large number of online incentives. With online sample, we can possibly fly in nature come from the online interaction between the limitation and the environment. And the interaction between the learning agent environment is extremely expensive in many applications. So every time you vary with online samples for every step, you make addition so it's an expensive process. Also, in the same, the critical applications such as in healthcare, part-time, driving, exploration, duration, learning can lead to unsafe behaviors. That would be really good. But in such, you may not have started to look into an alternative learning power balance for all primary settings. So in all primary settings, the agent will take advantage of any historical data so it's called lock interactions and then take advantage of that to learn without sparrows during the learning process. And this is something quite natural and reasonable things to do in practice because we usually have in fact, this is a historical data set available for many applications, in industrial process, you already have the data set, we collected before and we want to take advantage of that value. And also because now there's no need to explore the solution is during the learning process every time you make decision. So it's good to be safer, it's good to be more practical or safe in risk-coouted conditions. And more importantly, by utilizing the lock and diversity experience system, we can enable better generalization called diverse domains. So all of my learning probably will have step-row challenges. And this one is, with my endless compared to successful learning, then output is a policy, it's not tradition. So tradition entails uncertainty and uncertainty is as important as tradition and accuracy for decision-making problems. And in the sequential setting, it's a sense that the output and some of the past actions that you have to take. And in off-line learning setting, you cannot call us more data. So essentially, you cannot test the solution immediately in the online manner. So that's the point where we're going to go. And there are also the challenges of entrepreneurship and large-day space that are among the most sought-in-hide that you've got to get a little bit more information. It's all really interesting to happen to you, because now you learn from the recollected offline data. So the data distribution creeped from the little viewers are the top policy that you want to your outside. And this kind of scenario, a child playing, it out to the hound and factual reasoning, what it mentions, like what it might work to take a different course of the sequence of actions rather than the offline data. And to see this, we can take a quick look at this complete example. Let's say you mentioned a simple word where this can be ordered in pre-order and the task here is to find a sorted pass from the start stage, it will stay. And let's say in the off-line learning, we are even recollected data sets up to the title dejectories, proven and yellow one. And the task is how you will show the path from the start stage to the good stage, the iterative learning, make the counterfactual decision at marked by the dash line. So I'm kind of beginning to stitch together two types of juxtaposed. And that's my ability is something that supervised learning, limitation learning, camera. What is the kind of task? Also, more general, so the off-line learning, there's kind of the tension between the potential for improvement for taking out the distribution actions and the risk of making a mistake due to the acceleration. And the second most important challenge is large stage phase. And really that's the very, very common problems for nowadays, inspired the whole area of deep learning and deep repository. So in practice, the world is much more complicated and the most stage-heavy in overs. So for example, in the game of chess, the 10 power of homing states in the game of real 10 to 10 to 17 states. There is obvious that we cannot learn this the individual and become good at not never data for that. So to give it that we use functional approximation and essentially function approximation is that we approximate the large stage phase by a more with a more compact one. And the idea is to reduce the number of states into the number of parameters that are more managed. You keep your networks to, you can go to the population, you can last well. And most importantly, for reduced function approximation, you enable generalization to the central states. And that's the global idea that why you want to buy the deep learning algorithm. And the chosen function, the most interesting when this comes to design algorithm is essentially kind of intensive bias. So this might believe that the learner's environment is simple even though the state studies is large. So really, the most fundamental question that people have been working on in the hot-flannel setting central years is when a central system is hot-flannel and cost-borne. So they send, by sample of reason, it means that the number of central dot reply to learn an epsilon optimal policy needs to scale polypolling with a related currency. And importantly, it's really independent of the state space because now it has the large state space which could be enormous. So to tackle our last question and the answer we've been providing, let's start with a little bit more problem side of that. The problem side of the concept here is the mock-up system process or MTP. So the MTP consists of the following e-components. First in the state space test, the state space in the system, all the possible say, the world living in the action space, A, the system, or a feasible action that I can take when I'm in the state. All right, so essentially, there's not even a land of interaction between the learning agent and the environment until they stop the new episode. Even like a land of a game, when you play a game rather than we want functions, we describe how much the learner can, how much we want to learn that can get when it takes certain action in certain state. Transition, probability VH, it does cry how the world would move it. The probability of working to the next day, people got the world's current state and it takes a certain action. And the interaction goes up all the way to the learning agent, the environment is as far out. So, and then it climbs that H, the level of the takes of the action that you H, and then the world responds with some revolve function, revolve signal, it's good to call it to the revolve distribution. And then the world is moving naturally to the next day, a code is the relationship to probability and we keep going on point to throw the interaction move. And in the general, we're constantly studying the going to buy a policy that maximizes the value function. Value function of policy essentially, the total expected reward give you one or certain policy. And I want to mention several of the quantities we need that later or so. So, the optimal value function essentially, the maximum value over all possible policy. And the key value function of how deep I have that H is the total spending rewards give you all a positive pie starting from that H. And so, the next is the past 8. So, similarly, optimal queue is the optimal queue function over all possible policy. So, it's kind of a measure of the value of some action and state that's been before us on how to do that. And you want to be able to study it in the user's book. And again, in the offline setting, that we're questioning, we're not only giving a pre-collected data to read the pre-collected data is n, just actuales. The digital reference is nothing but a complex sequence of experiences from step one to step eight. And so, the knowledge wrapping would be final then here. So, I hope the present is pretty good. And the goal of the objective here is that, so now the option we are going to be taking the pre-collected data set. So, how do you want to talk to the city? And function class from SA to the rules? You can look here. What's the top of it? It's still in there. Thank you. So, the second function class, SA to the rules of the function class, the code out of intensive bias about how to see for what it can write. So, in my student, you can find intensive biobriage knowledge, the point. So, I hope the class and I want to output some policy pi hat. And what do I want to find pi hat? So, the value of pi hat is as high as possible. And intensively close to the optimal value with high probability. So, I hope the check function is clear. And then, now, offline learning with function approximately, the approximation, approximately one step for efficient learning. And in order to have central office and learning, we need two types of condition assumptions. The full type assumption is this code data coverage. Assumption, the data assumption essentially says, how well my policy, that, my behavior policy, mu, the mu is the policy that generated the off-line data. So, how, since I say an action, so that I can run, and then this don't kind of assumption on the policy, it's not a wise, it's not a wise, it's not. The second time assumption is code representation assumption. So, representation assumption is the assumption that, in general, how accurate my ability, my app is, a lot of work. So, in this case, how my function class can actually well across the list, the target value assumptions. So, we can get more into complete detail of this two type function, pi hat. And the following, the FQI, or fixed and QI resolution, so FQIs among the most classical algorithm for off-line and sending, and classical analysis of FQI requires some types of assumptions. So, first assumption, they call all policy constant probability, so it's been that they reply, the worst case density ratio between the state action distribution induced by an internal policy pi through the state action distribution induced by the behavior policy view. And then this, the sample capacity of FQI's fair wind, this quantity and this quantity, because if the viscosity is more than, essentially, for any action, any state action, A thus can be produced by pi, then I can see that into the, the off-line data. So, off-line data is informative enough that allow to learn about any asset state. So, this assumption is quite strong and intuitive. Sending kind of assumption in presentation assumption in real-lifeability, so real-lifeability has some of that, the target function, the true target function, you know, the function class, so it's mean that my inventory by that is correct. And real-lifeability is, is might and assumption, and usually we're happy to make that assumption because even for us, we're learning, we need that assumption anyway for business learning values. And the second time assumption is completeness, which is something very typical to reinforcement in setting. So, completely, completely say something like that. My function class is closed under the bare-man operator. So, completely is quite strong as assumption, but this is quite standard, you must be sharing. So, and several plus the FQI scales with the, the, the, the, the, the, the, the last thing function class, again, tone. And, and so, somehow this by a condition is actually necessary for the classical FQI, you can FQI is long to shoot diverge when it's this condition of bioletic. And this natural leads to the question now, is can you really desire better and better with better guarantee and must be assumption that this, this can last forever. Let me try to believe which is less head of this kind of question. And then, so essentially for simple, based on offline learning, we can have some trade off between data, complex assumption and resolution assumption. So, if you make a, a presentation assumption, it's a intention to strong, the data complex assumption. And, and let's see this how a look, resolution looks like, let's say I'm using as axis represent the representation, then I go from the left to the right, to angle in the direction of the weaker, to strong representation assumption. And then you, I use the y axis to represent the data complex assumption, then I go from the bottom to the top, meaning I go in the direction from the weaker to the strong assumption. And the favorite region is once you go down to this region, where this one in the, the place where you make a good guess assumption. So, if you are in, obviously somewhere like that, you're in the complex, constant, regularity and good prediction. Okay? And recent, by the way, by CA and general 2020, they only think we'll as built a assumption. But is that a strong data complex assumption, investment of security, and can we probably make the realizability assumption, then even with all policy and constant security, there's nothing you can do about all finally, because that's an anecdote we saw about by Foster at all, to try to change the scene. So, this is the final point, natural, so some representation assumption, a little stronger in realizability. And if I can make a weaker, single policy, constant, near a assumption, and using some kind of more strong, a recompression, naturally, they can type with an empty page, and see it all, even have a weaker assumption of data, then single policy, constant security, and they use a, a conflict as. So, the natural question now is that, how much we can push down for the battery of a racialization? And of course, you cannot do anything about this region, found it by any basic possible, and you want to make some kind of assumption when you get a realizability and a standard to make some companies assumption as well. And in fact, we can do, push it much further down, along the line data, call this assumption, and this is the part about the global, or generally, popular policy optimization framework, that we'll talk about, and people need. So, global can work with the data, and it was the real notion of data coverage, and this one, what we know, what a notion of policy has as well. And the most, the rest of the talk, talk about the policy, the kinds of all, and the ideas of the science, and the other, so, so, realizability is a weaker assumption than a conflict list. Right. So, usually, it's a setifying, so realize realizability problem. It will be possible in MPB, that let's do it. So, which line, which the right-hand battery, can be somewhere in a curve where it's straight out between both, but right now, it seems you want to push one, there be front-hater battery. You want to go to the left and to the bottom. Yeah, so it should be something in somewhere around that corner, a little bit, in between the first and second, you can do it, or I need it first, too. Oh, it's good. And the top first, too. Like, if you draw a curve, yeah, yeah. I feel like, are you kind of seeking a balance between two, or you are looking more, just for one direction, like just reduce the risk of holding it? I do, and I want to go as well. Both. That's what I can do. Okay, okay, okay. Okay. I hope they can push you from this. Yeah. But you cannot just, anything here, so, anyone can make it on that. There's still some other space, yeah. Like, I like to leave this, this is hard to do, it's a good space for each side. Uh-huh. But these are all good enough because I think that's the same if we are resolved anyway. So, any question on this? Okay, so let's understand, from to the batteries and ports, which type of voltage and can, you know, to improve that, let's view up some insulation without the no-per-neutral, let's put some simple example. So, now we can view the off-line learning as a transfer problem to train transfer from one region to the same source of the region to some target region. So, give you new, recall, mu is behavior policy. And, and policy that I want to compete against, I could be optimal policy. So, it's kind of transfer of current problem. And, let's make our life easier by mentioning the world's signal, to say that there are almost no times of state action as, state action as in the high-value, moisture, sweat, and state action as that, lowering the current by the, the track, okay? So, what cost of regularity coefficients from FQI say is that, so cost of regularity coefficient by overpassiveism about measuring the relationship. In the same time, the target distribution is very different from this fall away from the source of the distribution. It doesn't mean that off-line learning is hard. For some point in this case, so the, does the core, how do you do the distributions? Well, then they do not, how do you do it? If the density ratio is infinity. So, however, what you do is that I can just, learn the latest act of the front-side distribution, and then I can successfully keep going, that's the front-side, the top of the distribution. And I still learn that optimal value, on this one, and it's right at the bottom of policy. So, deep-high form of the view, not necessarily, it's a hard part. So the source distribution keeps changing. This case, that means it. The source, it's stationary. The source distribution, I mean, the whole MTP is the translation that we, but we distract the deep-high, so deep-high essentially the same action, it's the distribution you might buy by pie, and this is stationary, it's the distribution, it keeps it fixed, one of the distribution. So, it fits one distribution, this is stationary. And another point I want to make about the notion of this conference is that, it's a, asymmetric in nature. In the sense that, transferring for one distribution, then this, let me say it again, it says that, it could be easier to transfer through one distribution to another, but it's not necessary, easier to transfer back. So, for example, in this case, because the coverage of the source distribution, kind of lies covered the whole state action region, okay, I learned some value function, that's separate into assets, then this can be more reliable and applied to this case. But in the talking distribution, the support is, the coverage is core, and to the satisfaction pass. So, the one we know here is, can we call tolerance, which I spoke to the subject distribution? So, off-line learning is an asymmetric problem in nature. And therefore, the notion of data coverage, should capture that kind, does a asymmetric nature as well. So, these two kinds of, these two kinds of cases naturally in the multivariate and policy transfer coefficient, essentially the intimation in the policy transfer coefficient is that, I want to make some connection between the function at, have low-government error under the source distribution, how can it be applied, the low value gap under the topic distribution? Okay, so, so, let's say you've eaten a supremacy part of it. And all I'm asking here is that, how much error under the short distribution can try down the value gap under the topic distribution? And this relationship captures like two confidence, see five years I call the policy transfer factor, and grow five years I call the policy transfer as well. And they typically call policy transfer coefficient. And when I, sorry, when grow five and see five grow, then the high and new are far away from each other. So, the policy transfer coefficient has captured the continuum of how this transfer learning from new to part. We can see this in more countries, example, let's say in the case, the argument, like, now the, the distribution is the uniform over some circle of certain radius. And the function class is not in both the half spaces go through the origin. And in this study, actually, it's been confused exactly the quantity involved in the definition of transfer coefficient. The respective gap between X, R, and half under any company policy, D pi zero because the respective family was granted any function that the D pi is zero point five. And this gap is a policy when the policy transfer is following is zero, the distance. And the policy is transfer as Poland zero indicates that off-line learning is easy. And this kind of intuitive period because essentially, so I have a value function and any distribution, the respective value is zero point five. So there's nothing to learn here because the, any policy is optimal policies of there's nothing to do with any off-line learning or transfer. So that's why this, the, as Poland has a transfer, as Poland precisely captured that scenario. And again, the positive probability is infinity. So it's totally failed to capture the scenario. And when the specific case is really equal to one, then the policy policy factor that becomes so people, data, and the grocery measures come from one of the point three's work as well. So the grocery measures essentially the worst case gap between the respective value gap, the prime and the square, the respective square distance of the grocery function. And again, the seed price where it's more than cost-triggerty always. We can take a look at more concrete stuff or to complete your how, how precisely denoted this and how general this notion is. So row prime can be any fixed row like say from some point this take us the function class is a stress-hole function at T. So first, first function at T means that the function at T evaluates the zero for any input, smaller than T and evaluates to one for any input greater than T. But it's the steps function. And the deep-py and TV form is commotion over the domain and the TV is have a density as which is the power function with the up-to-row minus what? by the fixed row. And you can compute the rapidly in this take that's the expected value gap in the T-py and T-prime and the expected both the expected square last piece and make sure you actually an anti-prime scale with T when that's a prime over two low. And this prescience means that full strength of a total of the row when we're in row-makes are they what data they're the infinity. So essentially the point I want to make is that policy transfer coefficient can be much more general than the data was the biggest slide as well. And in fact if we throw the net system all the problems that's brought by the data coverage assumption the policy transfer is the most general notion of data coverage so is what the research and of course is much more general than all kinds of familiarity. And another beautiful part of the policy transfer coefficient is that it's captured the performance of the minimac and for offline learning in the general case. So let me briefly get into this detail. And let's say in high class is the policy trained by some other examples. It's point view. Okay. So let's make a standard pre-presentation assumption. Those are realized in complete and the theory is in this form in the minimac sense. For all possible offline learning algorithm. For all possible talking policy pie. For all possible MTPM and for all possible function class app. Such that the policy can flow from view to pie as roll and see for some physical and see. And so is that the functional class realizing the end of the process. So we can see the representation of the social we make earlier. Then this is the way you can get in any situation. And in minimac theory in sense of learning theory, this essentially say that any offline learning algorithm you cannot do better than the low-bout. And the low-bout says that there is this a learning algorithm. So if you can see that in this case, you can see that this is the function. And the function is this. And the function is this. So we can see that this is the function. So we can see that the function is the function. And the function is the physical function, but is the   produk touch you and this. So that is the work-out . empty piece in very idealized way. And then what you do is that you try to minimize the value function of the policy and the customer's name, how will the sets of sets is going possible empty piece. And if you do that for a very policy and you pick the best one, then you get to see that the value of the math main optimization problem is under S2M of the value of the optimal policy. So this kind of procedure can be viewed and naturally as an action credit framework. So I have the credit, which is carrying out the optimization control. So what I'm going to do is that even anything all by the way, the sets and the policy part in the past generation, I try to create the passing distance policy evaluation of the current policy and then pass to that value. So now the guy is so active and after the title, we see the have worked out the function and 20 input policy and they keep the set to do this. And let me get a little bit more into this. So this whole idea is generate all my policy optimization and the passing means the part is a key part here for the side of the target. So you have a data set, you have a fixed policy. And so the one to create the passing is an action name of the policy and the consideration for the meeting management for rent. And we can have, let me give you a way to solve it. We can use, we can use, it's going to bring you a constraint optimization, we're going to analyze optimizations and our policy is something to do. And we can get to this detail later. But the key point is to obtain pessimism in the initial states on this. And then the actor, the actor will start with some initial policy. Then it's passed to the current, the current is a given evaluation and then they give time to reduce the sequence of policy that get improved over time. And naturally, from the perspective of an actor, so actor essentially receive a sequence of adversarial objective function. So actor is performing all my learning and going because that's important to adversarial standard. And we can use the all my learning and learning set mirror to send what's on the whole plan to the actor. So the key point here is, all my learning can be designed and the all my learning and learning does, and the person is a policy design. So let me try to finish it. So let me really talk from the gradest conference, like the train optimization. The idea of train optimization is essentially directly from the design of the passing is it crazy. What I want to start with is that I start with the sets of all my learning functions that could have been generated the offline data. And then, among all these learning functions, I picked the one that had smallest value in the initial stage. And if you can see, let's try optimization, we allow to solve the contract optimization, which could be computationally intractable for the non-converse problems, which are general function, approximation. So instead, we can reframe this problem, meaning that we have to memorize optimization, review this, the purchase, and the multi-objective optimization problem. The first objective is that it wants to have small value in the initial stage, say the authentic world of functions without the small minimum error, and be just a wide realization. Something international. And the nothing about the realization, we don't have to solve the revolutionized optimization. Sorry, we don't have to solve the constraint optimization. So this could be amenable to more practical implementation in practice. And something even loud, then these two are going to be that we can completely avoid optimization problem by doing something. And sampling a function could be more efficient than doing optimization now on the function class in the sample case. And then what we do here is take a short, the procedure, procedure, which consists of two parts, the first part, the fact prior. And the secret to obtain pessimism here is very similar to at one term, which has the smallness of one, and I have one that I encourage the function to be sampled to have small value in the initial, in the initial stage. And second, it does like in what function is the very standard term in process design, and I want to have small family human. The miracle is that straight forward from the online learning, because we want to maximize the value function. At the same time, we don't want to give you too much from the previous array. So this allows you to actually a very similar update rules, which is an exponential waiting by the end of the year. And a few years ago, it was an objective question. And then the update rules in practice in the practice of space could be very similar, essentially, it's an all-like way to send changes to the system. So we have reviewed the model for learning and the new last problem. We use HATHA, the present tremor to solve this problem, which will function as a approximation. And we have three times of credits to write the credits on screen. And which is nice, but this is the whole framework. And when you use key, the price of $1.0.23, have the same guarantee. And the last guarantee is characterized by the policy transfer coefficient that we see earlier. And it's a little bit surprising because the biggest thing is that we only give the sub-bonding more value for the regularization optimization. And positive sampling is, I believe this is the first time a technique is an off-lighter. Because positive sampling used to give us exploration, use of the regularization. But in the service can be used in the off-lighter, except that it's not only that it can obtain the optical balance. So can you go back and show us what we have to change? Yeah, so in that view, can you just, you get the terms what we want to, once we can output? Yes, of course. So first, the C and rho here is the positive transfer coefficient. So C is positive transfer factor, and rho is positive transfer as common. So to C and rho, we have to write the duty of the ship into the data. H in the horizon, so the last moon land of interaction between the learning agent and the environment. And this is, you mentioned the length of the game when you play H with the game, which is how long it takes. D, where is it? And here is a number of trajectories from the off-lighter. So that's a number of samples you have from the regularize data. So if you guys more samples, the arrows, should drive it down. And D, in this case, assume the mention of the function class, let's say the complicit the function class, if the function class is complicit, then the bow gets invicator. But the benefits of complex function class, such as the neural network is that, this can capture the real world. So in real world, like you need to do, is that the blame models and neural network can capture the complex world, and then it will drive up a bit. So that we quickly go into some future extension of this, and then I can write this up. So, so far I'm talking about off-light learning, the kind of transfer learning problem from one policy to another. Now, what about transfer from one task to another task? So it's getting more challenging. So this is that the code, transfer learning property. And can you learn in the data scan environment, for example, using rising data with the environment, so that's the central question of transfer learning. And usually, transfer learning can be done via representation learning. So this is actually you see every day, that people train the whole network on the pre-collective data system, and then they freeze the first few layers and youth and that presentation. And later on, when they see the new task, they apply that presentation to one of the tasks faster. So representation can allow potential of knowledge. But in the, to take advantage of representation learning, we can consider the common functionality more for some point. Because MTP, we can see MTP at the most interesting representation and prediction. And see of this at one example, some previously mentioned, and representation is something complex, such as deep neural network, something that you refrain on the data, and the predictor is some linear predictor. When you fight, it's just by the last few layers on it. And that's physical, how just by learning it, you need a G4 task from B1 PT with N-Central, one task. And we ask the last thing that can be learned also in a policy for the target task, which is plus one, even that whole task, which is a common representation. So the most interesting thing is that we can do a great set of procedures that we generally do when they put all data on the song side using massive likelihood, and then we can use a learning representation for any downstream task. And this is only what we have been doing in the previous paper in New Years. And the thing is that multi-packed representation learning can be used to need learning, presentation, draw scratch for any target task. There's all the author learning algorithms as well, using right-based data neural networks, or the fact that we have a learning process, and so even though this author learning has been studying for a while, probably getting a little bit of a problem. And I want to mention the question that I think most is following. So color is transferable, and color is the one with both general notion now. Even that, so it can be derived up even better notion than the color transfer algorithms. So that's what I'm learning. And second, we have not really talked about computational aspects yet, or offline learning. And this is one of the most interesting things in our terms. The settings for talk about innovation only, but efficiency. So what kind of assumptions that can allow efficiency of offline learning? And the question about offline online is that we said in fundamental separation, which in two settings, and for the transform, we said the passive multi that can caress around the tranquilability between the task, more cheerful, and more simple, so efficient. And the favorite part is about going to be decide. When you come to actually decide to go on in them. And this kind of result have absorbed into the recent project that I've been split into, and this is a, or mental microvolts in AI. And other than that, that's kind of project. I also have some things going on on the presentation learning. And especially I use some more multi-action learning, the interaction, the intersection, which is much in learning and gave me a really calmness, very interesting. And very recently, I wrote it first in the tri-thalmers, and wanted to learn about the, I mean, wanting open answers, the fun, when to question about how.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Path to audio file\n",
    "audio_path = r\"C:\\Users\\nurja\\research\\Recording_fixed.wav\"\n",
    "\n",
    "# Transcribe the audio\n",
    "try:\n",
    "    result = model.transcribe(audio_path)\n",
    "    print(\"Transcription:\\n\", result[\"text\"])\n",
    "except Exception as e:\n",
    "    print(f\"Error during transcription: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a861b1-404b-4725-a7f6-8e76d22ed0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
